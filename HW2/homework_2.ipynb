{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /mnt/d/linux/env/lib/python3.5/site-packages (4.35.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.1, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "\n",
    "import numpy as np\n",
    "import numpy.linalg as npla\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve Exercise 3.4. Explain how you obtained the table. Your solution may be hand-written."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $R_{search}$ be the rv of reward when the robot is searching. <br>\n",
    "$P_{R_{search}}(r) = P[R_{search}= r]$ <br> \n",
    "<br>\n",
    "Let $R_{wait}$ be the rv of reward when the robot is waiting. <br>\n",
    "$P_{R_{wait}}(r) = P[R_{wait}= r]$ <br> \n",
    "<br>\n",
    "Let there distribution be s.t. <br>\n",
    "$E[R_{search}] = r_{search}$ <br>\n",
    "$E[R_{wait}] = r_{wait}$ <br>\n",
    "<br>\n",
    "Then, <br>\n",
    "\n",
    "\n",
    "| s    | a        | s'   | r  | p(s',r\\|s,a)                       | \n",
    "|:-----|:---------|:-----|:---|:----------------------------------:|\n",
    "| high | search   | high | r  | $\\alpha$ . $P_{R_{search}}(r)$     |\n",
    "| high | search   | low  | r  | $(1-\\alpha)$ . $P_{R_{search}}(r)$ | \n",
    "| high | wait     | high | r  | $P_{R_{wait}}(r)$                  | \n",
    "| low  | search   | high | -3 | $(1-\\beta)$                        |\n",
    "| low  | search   | low  | r  | $\\beta$ . $P_{R_{search}}(r)$      |\n",
    "| low  | wait     | low  | r  | $P_{R_{wait}}(r)$                  |\n",
    "| low  | recharge | high | 0  |  1.0                               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write code that solves the linear equations required to find v$_π$(s) and generate the values in the table in Figure 3.2. Note that the policy π picks all valid actions in a state with equal probability. Add comments to your code that explain all your steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid size\n",
    "n = 5\n",
    "#Number of states\n",
    "n_s = n*n\n",
    "#Number of actions\n",
    "n_a = 4\n",
    "#Position A\n",
    "A_r, A_c = 0, 1\n",
    "#Position A'\n",
    "A1_r, A1_c = 4, 1\n",
    "#Position B\n",
    "B_r, B_c = 0, 3\n",
    "#Position B'\n",
    "B1_r, B1_c = 2, 3\n",
    "#discount\n",
    "gamma = 0.9\n",
    "\n",
    "#Direction convention => 0:East, 1:North, 2:West, 3:South\n",
    "#Policy: First 2 coordinates signify state and the third signifies action\n",
    "#All actions equi-probable in all states\n",
    "policy = np.ones((n, n, n_a)) / n_a\n",
    "\n",
    "#Transition Fucntion: given current state and action taken returns new state(s1_c, s1_r) and reward(r) earned\n",
    "def transit(s_r, s_c, a):\n",
    "    s1_r, s1_c, r = -10, -10, -10\n",
    "    #Position A: leads to A' for all actions with +10 reward\n",
    "    if s_r == A_r and s_c == A_c:\n",
    "        s1_r, s1_c = A1_r, A1_c\n",
    "        r = 10\n",
    "    #Position B: leads to B' for all actions with +5 reward    \n",
    "    elif s_r == B_r and s_c == B_c:\n",
    "        s1_r, s1_c = B1_r, B1_c\n",
    "        r = 5\n",
    "    #East Boundary: going East -1 reward    \n",
    "    elif s_c == 0 and a == 0:\n",
    "        s1_r, s1_c = s_r, s_c\n",
    "        r = -1\n",
    "    #North Boundary: going North -1 reward    \n",
    "    elif s_r == 0 and a == 1:\n",
    "        s1_r, s1_c = s_r, s_c\n",
    "        r = -1\n",
    "    #West Boundary: going West -1 reward    \n",
    "    elif s_c == n-1 and a == 2:\n",
    "        s1_r, s1_c = s_r, s_c\n",
    "        r = -1\n",
    "    #South Boundary: going South -1 reward    \n",
    "    elif s_r == n-1 and a == 3:\n",
    "        s1_r, s1_c = s_r, s_c\n",
    "        r = -1\n",
    "    #Staying inside the grid with 0 reward\n",
    "    else:\n",
    "        #Going East\n",
    "        if a == 0:\n",
    "            s1_r, s1_c = s_r, s_c - 1\n",
    "        #Going North\n",
    "        elif a == 1:\n",
    "            s1_r, s1_c = s_r - 1, s_c\n",
    "        #Going West\n",
    "        elif a == 2:\n",
    "            s1_r, s1_c = s_r, s_c + 1\n",
    "        #Going South\n",
    "        else:\n",
    "            s1_r, s1_c = s_r + 1, s_c\n",
    "        #Gaining 0 reward\n",
    "        r = 0\n",
    "    \n",
    "    return s1_r, s1_c, r\n",
    "\n",
    "#An equation for each state\n",
    "eqn = np.zeros((n_s, n_s))\n",
    "#Constant for each equation\n",
    "c = np.zeros(n_s)\n",
    "\n",
    "#Row-Major Style\n",
    "for s_r in range(n):\n",
    "    for s_c in range(n):\n",
    "        #co-efficients of all v_pi(s')\n",
    "        weights = np.zeros((n, n))\n",
    "        #co-efficient of v_pi(s)\n",
    "        weights[s_r, s_c] = -1\n",
    "        #scalar additives in equation\n",
    "        scals = 0\n",
    "        \n",
    "        for a in range(n_a):\n",
    "            #Policy realisation\n",
    "            poly = policy[s_r, s_c, a]\n",
    "            #New states (s')\n",
    "            s1_r, s1_c, r = transit(s_r, s_c, a)\n",
    "            #co-efficient of v_pi(s')\n",
    "            weights[s1_r, s1_c] += poly * gamma\n",
    "            #scalars\n",
    "            scals +=  poly * r\n",
    "        #Storing equations\n",
    "        eqn[s_r*n + s_c] = weights.flatten(order='C')\n",
    "        c[s_r*n + s_c] = -scals\n",
    "        \n",
    "v_pi = npla.solve(eqn, c).reshape((n, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gridworld\n",
      "[[ 3.31  8.79  4.43  5.32  1.49]\n",
      " [ 1.52  2.99  2.25  1.91  0.55]\n",
      " [ 0.05  0.74  0.67  0.36 -0.4 ]\n",
      " [-0.97 -0.44 -0.35 -0.59 -1.18]\n",
      " [-1.86 -1.35 -1.23 -1.42 -1.98]]\n"
     ]
    }
   ],
   "source": [
    "print('Gridworld')\n",
    "print(np.round(v_pi, decimals=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve Exercises 3.15 and 3.16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$v_\\pi(s) = E_\\pi[G_t | S_t=s]$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve Exercises 3.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Continuous** <br>\n",
    "$v_\\pi(s) = E_\\pi[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} .... | S_t=s]$ <br>\n",
    "<br>\n",
    "Adding c to all rewards<br>\n",
    "<br>\n",
    "$v^c_\\pi(s)  = E_\\pi[(R_{t+1}+c) + \\gamma (R_{t+2}+c) + \\gamma^2 (R_{t+3}+c) .... | S_t=s]$ <br>\n",
    "$\\quad  = E_\\pi[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} .... | S_t=s] + E_\\pi[c + \\gamma c + \\gamma^2 c .... | S_t=s]$ <br><br>\n",
    "$\\quad  = v_\\pi(s) + \\frac{c}{1-\\gamma}$ <br>\n",
    "<br>\n",
    "The value function doesn't change relatively for states. Each of them have same scalar added to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve Exercises 3.16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Episodic** <br>\n",
    "$v_\\pi(s) = E_\\pi[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... + \\gamma^{n-1} R_{t+n} | S_t=s]$ <br>\n",
    "<br>\n",
    "Adding c to all rewards<br>\n",
    "<br>\n",
    "$v^c_\\pi(s)  = E_\\pi[(R_{t+1}+c) + \\gamma (R_{t+2}+c) + \\gamma^2 (R_{t+3}+c) + ... + \\gamma^{n-1} (R_{t+n}+c) | S_t=s]$ <br>\n",
    "$\\quad  = E_\\pi[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... + \\gamma^{n-1} R_{t+n} | S_t=s] + E_\\pi[c + \\gamma c + \\gamma^2 c + ... + \\gamma^{n-1} c | S_t=s]$ <br><br>\n",
    "$\\quad  = v_\\pi(s) + c \\frac{1-\\gamma^{n}}{1-\\gamma}$ <br>\n",
    "<br>\n",
    "The value function does change relatively for states. Now it depends upon the time($n$) after which the episode ends when started in state $s$. It increases as n increases. And as the time after which the episode ends for different start states, the value of the additive term changes and hence the value functions of the term aren't relatively same as before.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write code that generates the optimal state-value function and the optimal policy for the Gridworld in Figure 3.5. You want to solve the corresponding system of non-linear equations. Explain all your steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given an equation for v∗ in terms of q∗."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$q_*(s, a) = \\max\\limits_\\pi q_\\pi(s, a)$ <br>\n",
    "$v_*(s) = \\max\\limits_{a\\in A(s)} \\max\\limits_\\pi q_\\pi(s, a)$ <br>\n",
    "$v_*(s) = \\max\\limits_{a\\in A(s)} q_*(s, a)$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code policy iteration and value iteration (VI) to solve the Gridworld in Example 4.1. Your code must log output of each iteration. Pick up a few sample iterations to show policy evaluation and improvement at work. Similarly, show using a few obtained iterations that every iteration of VI improves the value function. Your code must include the fix to the bug mentioned in Exercise 4.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid Size\n",
    "n = 4\n",
    "#No. of states\n",
    "n_s = (n*n) - 2\n",
    "#Terminal State\n",
    "st_r, st_c = 0, 0\n",
    "#No. of actions\n",
    "n_a = 4\n",
    "#discount\n",
    "gamma = 1\n",
    "#convergence\n",
    "theta = 1e-10\n",
    "\n",
    "#Actions => 0:left, 1:up, 2:right, 3:down\n",
    "def transit_grid(s_r, s_c, a):\n",
    "    s1_r = -1\n",
    "    s1_c = -1\n",
    "    #Reward\n",
    "    r = -1\n",
    "    \n",
    "    #Terminal State\n",
    "    if s_r==0 and s_c==1 and a==0:\n",
    "        s1_r, s1_c = st_r, st_c\n",
    "    elif s_r==1 and s_c==0 and a==1:\n",
    "        s1_r, s1_c = st_r, st_c\n",
    "    elif s_r==n-1 and s_c==n-2 and a==2:\n",
    "        s1_r, s1_c = st_r, st_c\n",
    "    elif s_r==n-2 and s_c==n-1 and a==3:\n",
    "        s1_r, s1_c = st_r, st_c\n",
    "        \n",
    "    #Going outside the left boundary\n",
    "    elif s_c==0 and a==0:\n",
    "        s1_r, s1_c = s_r, s_c\n",
    "    #Going outside the top boundary\n",
    "    elif s_r==0 and a==1:\n",
    "        s1_r, s1_c = s_r, s_c\n",
    "    #Going outside the right boundary\n",
    "    elif s_c==n-1 and a==2:\n",
    "        s1_r, s1_c = s_r, s_c\n",
    "    #Going outside the botton boundary\n",
    "    elif s_r==n-1 and a==3:\n",
    "        s1_r, s1_c = s_r, s_c\n",
    "    \n",
    "    #Move left\n",
    "    elif a==0:\n",
    "        s1_r, s1_c = s_r, s_c-1\n",
    "    #Move up\n",
    "    elif a==1:\n",
    "        s1_r, s1_c = s_r-1, s_c\n",
    "    #Move right\n",
    "    elif a==2:\n",
    "        s1_r, s1_c = s_r, s_c+1\n",
    "    #Move down\n",
    "    elif a==3:\n",
    "        s1_r, s1_c = s_r+1, s_c\n",
    "\n",
    "    return s1_r, s1_c, r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bug mentioned in Exercise 4.4 is dealt by using numpy.argamx() which internally handles it, as it has follows a consistent convention when selecting between equal values. It always selects the one that has a lower index and since indexes of actions don't change in code, therefor it can't oscillate between equally favaourable policies as it will always choose the action with a lower index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(n_1, n_2, n_a, theta, gamma, transit):\n",
    "    \n",
    "    #Optimal Value Function\n",
    "    v = np.zeros((n_1, n_2))\n",
    "    #Optimal Policy\n",
    "    pi = np.ones((n_1, n_2, n_a))/n_a\n",
    "\n",
    "    i = 0\n",
    "    while True:\n",
    "        print('Iteration:', i)\n",
    "        #Policy Evaluation\n",
    "        print('Policy Evaluation')\n",
    "        while True:\n",
    "            #Improvement Measure\n",
    "            delta = 0\n",
    "            for s_r in range(n_1):\n",
    "                for s_c in range(n_2):\n",
    "                    #avoiding Terminal state\n",
    "                    if (s_r==0 and s_c==0) or (s_r==n_1-1 and s_c==n_2-1):\n",
    "                        continue\n",
    "                    #current Value function\n",
    "                    v_old = v[s_r, s_c]\n",
    "                    v_ = 0\n",
    "                    for a in range(n_a):    \n",
    "                        #new state and reward\n",
    "                        s1_r, s1_c, r = transit(s_r, s_c, a)\n",
    "                        #Expected Return\n",
    "#                         print('s', s_r, s_c, ', a', a-5, 's1', s1_r, s1_c)\n",
    "                        v_ += pi[s_r, s_c, a] * (r + (gamma * v[s1_r, s1_c]))\n",
    "                    #update Value function\n",
    "                    v[s_r, s_c] = v_\n",
    "                    #max-improvement\n",
    "                    delta = max(delta, abs(v_old - v[s_r, s_c]))\n",
    "            print('del', delta)\n",
    "            if delta < theta:\n",
    "                print('delta < theta')\n",
    "                break\n",
    "        print('Value Function')\n",
    "        print(v)\n",
    "\n",
    "        #Policy Improvement\n",
    "        print('Policy Improvement')\n",
    "        stable_poly = True\n",
    "        for s_r in range(n_1):\n",
    "            for s_c in range(n_2):\n",
    "                #avoiding Terminal state\n",
    "                if (s_r==0 and s_c==0) or (s_r==n_1-1 and s_c==n_2-1):\n",
    "                    continue\n",
    "                #action from old policy\n",
    "                a_old = np.random.choice(n_a, p=pi[s_r, s_c])\n",
    "                #exected returns on all actions\n",
    "                g = np.zeros(n_a)\n",
    "                for a in range(n_a):\n",
    "                    #new state and reward\n",
    "                    s1_r, s1_c, r = transit(s_r, s_c, a)\n",
    "                    #expected return\n",
    "                    g[a] = r + (gamma * v[s1_r, s1_c])            \n",
    "                #best action\n",
    "                a_star = np.argmax(g)\n",
    "                #update policy\n",
    "                pi[s_r, s_c] *= 0.\n",
    "                pi[s_r, s_c, a_star] = 1.\n",
    "                #stability check\n",
    "                if a_old != a_star:\n",
    "                    stable_poly = False\n",
    "\n",
    "        print('Policy')\n",
    "        print(np.argmax(pi, axis=2))\n",
    "\n",
    "        if stable_poly:\n",
    "            print()\n",
    "            print('Policy Stable')\n",
    "            break\n",
    "        i+=1\n",
    "        print()\n",
    "\n",
    "    return v, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Policy Evaluation\n",
      "Value Function\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "Policy Improvement\n",
      "Policy\n",
      "[[0 0 0 0]\n",
      " [1 1 0 3]\n",
      " [1 1 2 3]\n",
      " [1 2 2 0]]\n",
      "\n",
      "Iteration: 1\n",
      "Policy Evaluation\n",
      "Value Function\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "Policy Improvement\n",
      "Policy\n",
      "[[0 0 0 0]\n",
      " [1 0 0 3]\n",
      " [1 0 2 3]\n",
      " [1 2 2 0]]\n",
      "\n",
      "Iteration: 2\n",
      "Policy Evaluation\n",
      "Value Function\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "Policy Improvement\n",
      "Policy\n",
      "[[0 0 0 0]\n",
      " [1 0 0 3]\n",
      " [1 0 2 3]\n",
      " [1 2 2 0]]\n",
      "\n",
      "Policy Stable\n",
      "v_star\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "pi_star\n",
      "[[0 0 0 0]\n",
      " [1 0 0 3]\n",
      " [1 0 2 3]\n",
      " [1 2 2 0]]\n"
     ]
    }
   ],
   "source": [
    "v_star, pi_star = policy_iteration(n_1=n, n_2=n, n_a=n_a, theta=theta, gamma=gamma, transit=transit_grid)\n",
    "print('v_star')\n",
    "print(v_star)\n",
    "print()\n",
    "print('pi_star')\n",
    "print(np.argmax(pi_star, axis=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Value Function\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "\n",
      "Iteration: 1\n",
      "Value Function\n",
      "[[ 0. -1. -2. -2.]\n",
      " [-1. -2. -2. -2.]\n",
      " [-2. -2. -2. -1.]\n",
      " [-2. -2. -1.  0.]]\n",
      "\n",
      "Iteration: 2\n",
      "Value Function\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "Iteration: 3\n",
      "Value Function\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "#Optimal Value Function\n",
    "v = np.zeros((n, n))\n",
    "#Optimal Policy\n",
    "pi = np.zeros((n, n))\n",
    "#precision\n",
    "theta = 1e-10\n",
    "\n",
    "i=0\n",
    "while True:\n",
    "    print('Iteration:', i)\n",
    "    #Improvement measure\n",
    "    delta = 0\n",
    "    \n",
    "    for s_r in range(n):\n",
    "        for s_c in range(n):\n",
    "            \n",
    "            #avoiding Terminal state\n",
    "            if (s_r==0 and s_c==0) or (s_r==n-1 and s_c==n-1):\n",
    "                continue\n",
    "            \n",
    "            #current Value function\n",
    "            v_old = v[s_r, s_c]\n",
    "\n",
    "            #Policy Evaluation\n",
    "            #Expected Return for each action\n",
    "            g = np.zeros(n_a)\n",
    "            for a in range(n_a):\n",
    "                #new state and reward\n",
    "                s1_r, s1_c, r = transit(s_r, s_c, a)\n",
    "                #expected return\n",
    "                g[a] = r + (gamma * v[s1_r, s1_c])\n",
    "            \n",
    "            #Policy Improvement\n",
    "            #Update value function\n",
    "            v[s_r, s_c] = np.amax(g)\n",
    "            #max-improvement\n",
    "            delta = max(delta, abs(v_old - v[s_r, s_c]))\n",
    "    \n",
    "    print('Value Function')\n",
    "    print(v)\n",
    "\n",
    "    if delta < theta:\n",
    "        break\n",
    "    \n",
    "    i+=1\n",
    "    print()\n",
    "        \n",
    "#Building optimal policy\n",
    "for s_r in range(n):\n",
    "    for s_c in range(n):\n",
    "        \n",
    "        #avoiding Terminal state\n",
    "        if (s_r==0 and s_c==0) or (s_r==n-1 and s_c==n-1):\n",
    "            continue\n",
    "        \n",
    "        #Expected Return for each action\n",
    "        g = np.zeros(n_a)\n",
    "        for a in range(n_a):\n",
    "            #new state and reward\n",
    "            s1_r, s1_c, r = transit(s_r, s_c, a)\n",
    "            #expected return\n",
    "            g[a] = r + (gamma * v[s1_r, s1_c])\n",
    "\n",
    "        #Policy Improvement\n",
    "        #Update value function\n",
    "        pi[s_r, s_c] = np.argmax(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_star\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "pi_star\n",
      "[[0. 0. 0. 0.]\n",
      " [1. 0. 0. 3.]\n",
      " [1. 0. 2. 3.]\n",
      " [1. 2. 2. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print('v_star')\n",
    "print(v)\n",
    "print()\n",
    "print('pi_star')\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code exercise 4.7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max cars allowed \n",
    "n_1, n_2 = 21, 21\n",
    "\n",
    "#Expectations\n",
    "lren_1 = 3\n",
    "lren_2 = 4\n",
    "lret_1 = 3\n",
    "lret_2 = 2\n",
    "\n",
    "#Rewards\n",
    "r_mov = -2\n",
    "r_ren = 10\n",
    "\n",
    "#number of actions: -5 to 5 cars moved (a to b is positive)\n",
    "n_a = 11\n",
    "\n",
    "#discount\n",
    "gamma = 0.9\n",
    "\n",
    "#convergence\n",
    "that = 0.1\n",
    "\n",
    "def transit_jack(s_1, s_2, a):\n",
    "    \n",
    "    ns_1, ns_2, r = s_1, s_2, 0\n",
    "    \n",
    "    #If none of the 2 stations are closed cars can be moved \n",
    "    if s_1 !=0 and s_2!=0:   \n",
    "        #number of cars to move\n",
    "        c = a-5\n",
    "        #cars moved\n",
    "        c = np.clip(c, s_1-(n_1-1), s_1)\n",
    "        c = np.clip(c, -s_2, n_2-1-s_2)\n",
    "        ns_1, ns_2 = s_1-c, s_1+c\n",
    "        #reward\n",
    "        r = abs(c)*r_mov\n",
    "#         print('moved', c)\n",
    "    \n",
    "    #cars returned\n",
    "    if s_1 != 0:\n",
    "        ret_1 = min(n_1-1-ns_1, np.random.poisson(lret_1))\n",
    "        ns_1 += ret_1\n",
    "#         print('return_1', ret_1)\n",
    "    \n",
    "    if s_2 != 0:\n",
    "        ret_2 = min(n_2-1-ns_2, np.random.poisson(lret_2))\n",
    "        ns_2 += ret_2\n",
    "#         print('return_2', ret_2)\n",
    "\n",
    "    #cars rented\n",
    "    if s_1 != 0:\n",
    "        ren_1 = min(ns_1, np.random.poisson(lren_1))\n",
    "        ns_1 -= ren_1\n",
    "        r += r_ren * ren_1\n",
    "#         print('rented_1', ren_1)\n",
    "    \n",
    "    if s_2 != 0:\n",
    "        ren_2 = min(ns_2, np.random.poisson(lren_2))\n",
    "        ns_2 -= ren_2\n",
    "        r += r_ren * ren_2\n",
    "#         print('rented_2', ren_2)\n",
    "            \n",
    "    return ns_1, ns_2, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Policy Evaluation\n",
      "del 214.46386400721158\n",
      "del 168.50474139919842\n",
      "del 132.9716447278182\n",
      "del 96.70121172165733\n",
      "del 79.46738681194293\n",
      "del 69.51029164990166\n",
      "del 73.4947050536091\n",
      "del 70.55151778125779\n",
      "del 71.0153386148258\n",
      "del 74.074047976299\n",
      "del 57.33831233318966\n",
      "del 86.7575404839203\n",
      "del 74.63451359691719\n",
      "del 77.35645791384775\n",
      "del 93.84760058239749\n",
      "del 63.28643453954663\n",
      "del 79.50664785105212\n",
      "del 76.49035173244877\n",
      "del 70.58236947925796\n",
      "del 64.27429492043082\n",
      "del 84.6853335254616\n",
      "del 71.55538249004513\n",
      "del 74.04521022637061\n",
      "del 57.524124265074136\n",
      "del 65.27682343244044\n",
      "del 72.05818058224918\n",
      "del 64.77702433798414\n",
      "del 62.60711647064937\n",
      "del 66.1511254292123\n",
      "del 67.70226405765703\n",
      "del 80.2850770481936\n",
      "del 76.95705665758567\n",
      "del 60.24082373771171\n",
      "del 65.55064399436895\n",
      "del 67.47799877224531\n",
      "del 77.24465073334824\n",
      "del 66.23456166229448\n",
      "del 84.9551452818614\n",
      "del 92.2582433166258\n",
      "del 95.60112649234736\n",
      "del 64.08340601400175\n",
      "del 71.33739517256419\n",
      "del 72.8803732788\n",
      "del 69.45953154319443\n",
      "del 89.6499198758425\n",
      "del 76.54306652685682\n",
      "del 61.17830626334646\n",
      "del 79.06147092599036\n",
      "del 78.63738564275354\n",
      "del 58.7012681758242\n",
      "del 73.64222466204006\n",
      "del 62.82813616972163\n",
      "del 75.95773394719893\n",
      "del 75.17060542761607\n",
      "del 69.76639449265252\n",
      "del 83.01260822613341\n",
      "del 67.5517888156827\n",
      "del 72.18715254953185\n",
      "del 63.59251272050619\n",
      "del 72.1102638550251\n",
      "del 68.93231206922184\n",
      "del 69.23627920496432\n",
      "del 80.43243875957938\n",
      "del 79.51490417416423\n",
      "del 72.21348810447907\n",
      "del 63.67537078461061\n",
      "del 54.897387713110675\n",
      "del 71.23023968559971\n",
      "del 73.6371763579296\n",
      "del 73.11588270310588\n",
      "del 81.6622419994523\n",
      "del 75.87268637045582\n",
      "del 68.01297087613386\n",
      "del 78.25549610900683\n",
      "del 84.01122658725866\n",
      "del 74.57843486300445\n",
      "del 75.79417773680495\n",
      "del 80.41998430478844\n",
      "del 75.15052802804217\n",
      "del 87.95621926188352\n",
      "del 77.66382221802118\n",
      "del 86.36874019070774\n",
      "del 87.46706507897545\n",
      "del 82.29128224261561\n",
      "del 75.90821858793437\n",
      "del 73.27127098473795\n",
      "del 76.1240487108123\n",
      "del 75.3536823822298\n",
      "del 75.40414811185042\n",
      "del 67.55998578989369\n",
      "del 80.91614565209375\n",
      "del 70.42825628706612\n",
      "del 63.9871368766878\n",
      "del 71.72395990168539\n",
      "del 63.05834760980599\n",
      "del 74.93990342504324\n",
      "del 80.43277668842694\n",
      "del 59.254108465212425\n",
      "del 67.41908795940537\n",
      "del 89.96648030301839\n",
      "del 82.30818430867402\n",
      "del 69.65727874148348\n",
      "del 76.65782823595322\n",
      "del 68.08298827502415\n",
      "del 68.07454931266074\n"
     ]
    }
   ],
   "source": [
    "v_star, pi_star = policy_iteration(n_1=n_1, n_2=n_2, n_a=n_a, theta=theta, gamma=gamma, transit=transit_jack)\n",
    "print('v_star')\n",
    "print(v_star)\n",
    "print()\n",
    "print('pi_star')\n",
    "print(np.argmax(pi_star, axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "583.2px",
    "left": "44px",
    "top": "110.8px",
    "width": "223.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
